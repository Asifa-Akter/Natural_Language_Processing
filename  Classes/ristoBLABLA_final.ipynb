{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "URL = 'https://www.ristoattrezzature.com/vetrina-congelatore-gelateria-verticale-con-anta-in-vetro-417-lt-18-25-c-66-7x62x201-8h-cm.html'\n",
    "page = requests.get(URL)\n",
    "\n",
    "soup = BeautifulSoup(page.content, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_div = soup.find('div',{'id':'Product-Accordion--Download'})\n",
    "link_pdf = pdf_div.find('a')\n",
    "href = link_pdf.get('href')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### print the second table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.ristoattrezzature.com/bspk-catalog/product_pdf/index/id/22327/\n",
      "['Alimentazione', 'Elettrico ']\n",
      "['Capacità ', '417 Lt']\n",
      "['Classe energetica', 'C ']\n",
      "['Consumo elettrico ', '4.68 Kw/24h']\n",
      "['Gas refrigerante', 'R290 ']\n",
      "['Peso', '108 kg']\n",
      "['Peso lordo', '116 kg']\n",
      "['Refrigerazione', 'statico ']\n",
      "[\"Temperatura d'esercizio\", '-18 -25 °C']\n"
     ]
    }
   ],
   "source": [
    "print(href)\n",
    "all_tables = soup.find_all('table')\n",
    "if len(all_tables)>= 2: \n",
    "    second_table =all_tables[1]\n",
    "    rows = second_table.find_all('tr')\n",
    "    for row in rows:\n",
    "        col = row.find_all('td')\n",
    "        col_data = [col.get_text() for col in col]\n",
    "        print(col_data)\n",
    "else:\n",
    "    print('table none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the both table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dimensioni esterne', '667x620x2018 mm']\n",
      "['Dimensioni imballo', '710x645x2110 mm']\n",
      "['Dimensioni interne', '547x490x1553 mm']\n",
      "['Alimentazione', 'Elettrico ']\n",
      "['Capacità ', '417 Lt']\n",
      "['Classe energetica', 'C ']\n",
      "['Consumo elettrico ', '4.68 Kw/24h']\n",
      "['Gas refrigerante', 'R290 ']\n",
      "['Peso', '108 kg']\n",
      "['Peso lordo', '116 kg']\n",
      "['Refrigerazione', 'statico ']\n",
      "[\"Temperatura d'esercizio\", '-18 -25 °C']\n"
     ]
    }
   ],
   "source": [
    "all_tables = soup.find_all('table')\n",
    "\n",
    "if len(all_tables) >= 2:\n",
    "    for table_index in range(2):  # Iterate through the first two tables\n",
    "        current_table = all_tables[table_index]\n",
    "        rows = current_table.find_all('tr')\n",
    "        for row in rows:\n",
    "            col = row.find_all('td')\n",
    "            col_data = [col.get_text() for col in col]\n",
    "            print(col_data)\n",
    "else:\n",
    "    print('At least two tables not found')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read the link from csv , scrape the pdf link and second table  then save it to a new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def remove_utf8_bom(text):\n",
    "    # Remove UTF-8 BOM character if present\n",
    "    if text.startswith('\\ufeff'):\n",
    "        text = text[1:]\n",
    "    return text\n",
    "\n",
    "def scrape_link(link):\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    pdf_div = soup.find('div', {'id': 'Product-Accordion--Download'})\n",
    "    link_pdf = pdf_div.find('a')['href']\n",
    "\n",
    "    all_tables = soup.find_all('table')\n",
    "    if len(all_tables) >= 2:\n",
    "        second_table = all_tables[1]\n",
    "        rows = second_table.find_all('tr')\n",
    "        details_info = []\n",
    "        for row in rows:\n",
    "            col = row.find_all('td')\n",
    "            col_data = [col.get_text() for col in col]\n",
    "            details_info.append(col_data)\n",
    "        return link, link_pdf, details_info\n",
    "    else:\n",
    "        return link, link_pdf, []\n",
    "\n",
    "# Read the list of links from the CSV file\n",
    "links = []\n",
    "with open('/Users/asifa/Desktop/Asifa/NLP KRISH/Metro_asifa.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        links.append(remove_utf8_bom(row[0]))\n",
    "\n",
    "# Scrape each link and store the results in a list\n",
    "results = []\n",
    "for link in links:\n",
    "    link, pdf_link, details_info = scrape_link(link)\n",
    "    results.append((link, pdf_link, details_info))\n",
    "\n",
    "# Write the results to a new CSV file\n",
    "with open('output.csv', 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['link', 'pdf_link', 'details_info'])\n",
    "    for link, pdf_link, details_info in results:\n",
    "        csv_writer.writerow([link, pdf_link, details_info])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def remove_utf8_bom(text):\n",
    "    # Remove UTF-8 BOM character if present\n",
    "    if text.startswith('\\ufeff'):\n",
    "        text = text[1:]\n",
    "    return text\n",
    "\n",
    "def scrape_link(link):\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    pdf_div = soup.find('div', {'id': 'Product-Accordion--Download'})\n",
    "    link_pdf = pdf_div.find('a')['href']\n",
    "\n",
    "    all_tables = soup.find_all('table')\n",
    "    if len(all_tables) >= 2:\n",
    "        second_table = all_tables[1]\n",
    "        rows = second_table.find_all('tr')\n",
    "        details_info = {}\n",
    "        for row in rows:\n",
    "            col = row.find_all('td')\n",
    "            col_data = [col.get_text().strip() for col in col]\n",
    "            details_info[col_data[0]] = col_data[1]\n",
    "        return link, link_pdf, details_info\n",
    "    else:\n",
    "        return link, link_pdf, {}\n",
    "\n",
    "# Read the list of links from the CSV file\n",
    "links = []\n",
    "with open('/Users/asifa/Desktop/Asifa/NLP KRISH/Metro_asifa.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        links.append(remove_utf8_bom(row[0]))\n",
    "\n",
    "# Scrape each link and store the results in a list\n",
    "results = []\n",
    "for link in links:\n",
    "    link, pdf_link, details_info = scrape_link(link)\n",
    "    results.append((link, pdf_link, details_info))\n",
    "\n",
    "# Write the results to a new CSV file\n",
    "with open('Ale_output.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['link', 'pdf_link', 'details_info'])\n",
    "    for link, pdf_link, details_info in results:\n",
    "        csv_writer.writerow([link, pdf_link, details_info])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data written to output.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "# Read the input CSV file\n",
    "input_file = '/Users/asifa/Desktop/Asifa/NLP KRISH/Ale_output.csv'  # Replace with your input file name\n",
    "output_file = 'output.csv'  # Replace with your output file name\n",
    "\n",
    "data = []\n",
    "\n",
    "# Read the CSV file\n",
    "with open(input_file, 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    header = next(csv_reader)  # Read the header\n",
    "    data = list(csv_reader)  # Read the rest of the data\n",
    "\n",
    "# Prepare the header for the new CSV\n",
    "new_header = header[:-1]  # Exclude the third column\n",
    "info_dict = {}  # To store the third column values as a dictionary\n",
    "\n",
    "for row in data:\n",
    "    # Convert the third column string to a dictionary using ast.literal_eval\n",
    "    info_dict = ast.literal_eval(row[-1])\n",
    "\n",
    "# Add keys as new columns to the header\n",
    "new_header.extend(info_dict.keys())\n",
    "\n",
    "# Extract and prepare the data for the new CSV\n",
    "new_data = []\n",
    "\n",
    "for row in data:\n",
    "    # Convert the third column string to a dictionary using ast.literal_eval\n",
    "    info_dict = ast.literal_eval(row[-1])\n",
    "    new_row = row[:-1]  # Exclude the third column\n",
    "\n",
    "    # Add values corresponding to each key in the info_dict\n",
    "    for key in info_dict.keys():\n",
    "        new_row.append(info_dict.get(key, ''))\n",
    "\n",
    "    new_data.append(new_row)\n",
    "\n",
    "# Write the new CSV with transformed data\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(new_header)\n",
    "    csv_writer.writerows(new_data)\n",
    "\n",
    "print(f\"Transformed data written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Organize the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'keys'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m data:\n\u001b[1;32m     19\u001b[0m     info_dict \u001b[39m=\u001b[39m ast\u001b[39m.\u001b[39mliteral_eval(row[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[0;32m---> 20\u001b[0m     all_keys\u001b[39m.\u001b[39mupdate(info_dict\u001b[39m.\u001b[39;49mkeys())\n\u001b[1;32m     22\u001b[0m \u001b[39m# Prepare the header for the new CSV\u001b[39;00m\n\u001b[1;32m     23\u001b[0m new_header \u001b[39m=\u001b[39m header[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]  \u001b[39m# Exclude the third column\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'keys'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "# Read the input CSV file\n",
    "input_file = '/Users/asifa/Desktop/Asifa/NLP KRISH/output.csv'\n",
    "output_file = 'output_new.csv'\n",
    "\n",
    "data = []\n",
    "\n",
    "# Read the CSV file\n",
    "with open(input_file, 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    header = next(csv_reader)  # Read the header\n",
    "    data = list(csv_reader)  # Read the rest of the data\n",
    "\n",
    "# Extract unique keys from the third column dictionaries\n",
    "all_keys = set()\n",
    "for row in data:\n",
    "    info_dict = ast.literal_eval(row[-1])\n",
    "    all_keys.update(info_dict.keys())\n",
    "\n",
    "# Prepare the header for the new CSV\n",
    "new_header = header[:-1]  # Exclude the third column\n",
    "new_header.extend(all_keys)  # Add unique keys as new columns\n",
    "\n",
    "# Extract and prepare the data for the new CSV\n",
    "new_data = []\n",
    "\n",
    "for row in data:\n",
    "    info_dict = ast.literal_eval(row[-1])\n",
    "    new_row = row[:-1]  # Exclude the third column\n",
    "\n",
    "    # Add values corresponding to each key in the new header\n",
    "    for key in all_keys:\n",
    "        new_row.append(info_dict.get(key, ''))\n",
    "\n",
    "    new_data.append(new_row)\n",
    "\n",
    "# Write the new CSV with transformed data\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(new_header)\n",
    "    csv_writer.writerows(new_data)\n",
    "\n",
    "print(f\"Transformed data written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def remove_utf8_bom(text):\n",
    "    # Remove UTF-8 BOM character if present\n",
    "    if text.startswith('\\ufeff'):\n",
    "        text = text[1:]\n",
    "    return text\n",
    "\n",
    "def scrape_link(link):\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    pdf_div = soup.find('div', {'id': 'Product-Accordion--Download'})\n",
    "    link_pdf = pdf_div.find('a')['href']\n",
    "    \n",
    "\n",
    "    all_tables = soup.find_all('table')\n",
    "\n",
    "    if len(all_tables) >= 2:\n",
    "        details_info = []\n",
    "        for table_index in range(2):  # Iterate through the first two tables\n",
    "            current_table = all_tables[table_index]\n",
    "            rows = current_table.find_all('tr')\n",
    "            table_data = []  # Collect data for each table\n",
    "            for row in rows:\n",
    "                col = row.find_all('td')\n",
    "                col_data = [col.get_text() for col in col]\n",
    "                table_data.append(col_data)\n",
    "            details_info.append(table_data)  # Append table data to the main details_info list\n",
    "        return link_pdf, details_info\n",
    "    else:\n",
    "        return link_pdf, []\n",
    "\n",
    "\n",
    "# Read the list of links from the CSV file\n",
    "links = []\n",
    "with open('/Users/asifa/Desktop/Asifa/NLP KRISH/Metro_asifa.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        links.append(remove_utf8_bom(row[0]))\n",
    "\n",
    "# Scrape each link and store the results in a list\n",
    "results = []\n",
    "for link in links:\n",
    "    pdf_link, details_info = scrape_link(link)\n",
    "    results.append((pdf_link, details_info))\n",
    "\n",
    "# Write the results to a new CSV file\n",
    "with open('output.csv', 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['pdf_links', 'details_info'])\n",
    "    for pdf_link, details_info in results:\n",
    "        csv_writer.writerow([pdf_link, details_info])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### read the link from csv , scrape the pdf link and both table  then save it to a new csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def remove_utf8_bom(text):\n",
    "    # Remove UTF-8 BOM character if present\n",
    "    if text.startswith('\\ufeff'):\n",
    "        text = text[1:]\n",
    "    return text\n",
    "\n",
    "def scrape_tables(table):\n",
    "    rows = table.find_all('tr')\n",
    "    details_info = {}\n",
    "    for row in rows:\n",
    "        col = row.find_all('td')\n",
    "        if len(col) >= 2:  # Ensure there are at least two columns\n",
    "            col_data = [col.get_text().strip() for col in col]\n",
    "            details_info[col_data[0]] = col_data[1]\n",
    "    return details_info\n",
    "\n",
    "def scrape_link(link):\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "\n",
    "    pdf_div = soup.find('div', {'id': 'Product-Accordion--Download'})\n",
    "    link_pdf = pdf_div.find('a')['href']\n",
    "\n",
    "    all_tables = soup.find_all('table')\n",
    "    details_info_list = []\n",
    "\n",
    "    for table in all_tables:\n",
    "        details_info = scrape_tables(table)\n",
    "        if details_info:\n",
    "            details_info_list.append(details_info)\n",
    "\n",
    "    return link, link_pdf, details_info_list\n",
    "\n",
    "# Read the list of links from the CSV file\n",
    "links = []\n",
    "with open('/Users/asifa/Desktop/Asifa/NLP KRISH/Metro_asifa.csv', 'r') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    for row in csv_reader:\n",
    "        links.append(remove_utf8_bom(row[0]))\n",
    "\n",
    "# Scrape each link and store the results in a list\n",
    "results = []\n",
    "for link in links:\n",
    "    link, pdf_link, details_info = scrape_link(link)\n",
    "    results.append((link, pdf_link, details_info))\n",
    "\n",
    "# Write the results to a new CSV file\n",
    "with open('Ale_output.csv', 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(['link', 'pdf_link', 'details_info'])\n",
    "    for link, pdf_link, details_info_list in results:\n",
    "        csv_writer.writerow([link, pdf_link, details_info_list])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key: link, Number of values: 118\n",
      "Key: pdf_link, Number of values: 118\n",
      "Key: Dimensioni esterne, Number of values: 118\n",
      "Key: Dimensioni imballo, Number of values: 73\n",
      "Key: Dimensioni interne, Number of values: 40\n",
      "Key: Alimentazione, Number of values: 111\n",
      "Key: Capacità, Number of values: 96\n",
      "Key: Classe energetica, Number of values: 61\n",
      "Key: Consumo elettrico, Number of values: 45\n",
      "Key: Gas refrigerante, Number of values: 106\n",
      "Key: Peso, Number of values: 65\n",
      "Key: Peso lordo, Number of values: 60\n",
      "Key: Refrigerazione, Number of values: 77\n",
      "Key: Temperatura d'esercizio, Number of values: 111\n",
      "Key: Spessore coibentazione, Number of values: 5\n",
      "Key: Capacità netta, Number of values: 15\n",
      "Key: Classe climatica, Number of values: 7\n",
      "Key: Frequenza, Number of values: 75\n",
      "Key: Materiale, Number of values: 5\n",
      "Key: Potenza Elettrica, Number of values: 91\n",
      "Key: Sbrinamento, Number of values: 11\n",
      "Key: Termostato, Number of values: 26\n",
      "Key: Voltaggio, Number of values: 82\n",
      "Key: In dotazione, Number of values: 48\n",
      "Key: Capacità vaschette, Number of values: 24\n",
      "Key: Rumorosità, Number of values: 10\n",
      "Key: Temperatura ambiente massima, Number of values: 28\n",
      "Key: Compressore, Number of values: 8\n",
      "Key: Capacità fette, Number of values: 2\n",
      "Key: Timer, Number of values: 2\n",
      "Key: Potenza Gas, Number of values: 3\n",
      "Key: Dimensioni superficie di cottura, Number of values: 1\n",
      "Key: Ripiani, Number of values: 7\n",
      "Key: Dimensioni griglie, Number of values: 1\n",
      "Key: Distanza ripiani, Number of values: 2\n",
      "Key: Capacità bottiglie, Number of values: 2\n",
      "Key: Dimensione tetto, Number of values: 10\n",
      "Key: Dimensioni ripiano, Number of values: 10\n",
      "Key: Piano espositivo, Number of values: 10\n",
      "Key: Capacità vasca, Number of values: 1\n",
      "Key: Peso netto, Number of values: 6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "csv_file = '/Users/asifa/Desktop/Asifa/NLP KRISH/Ale_output.csv'\n",
    "df = pd.read_csv(csv_file)\n",
    "\n",
    "# Convert the string representation of lists to actual lists\n",
    "df['details_info'] = df['details_info'].apply(eval)\n",
    "\n",
    "# Initialize an empty dictionary to store the extracted data\n",
    "data_dict = {'link': [], 'pdf_link': []}\n",
    "\n",
    "# Extract data from the list of dictionaries\n",
    "for item in df['details_info']:\n",
    "    item_dict = {}\n",
    "    for sub_dict in item:\n",
    "        item_dict.update(sub_dict)\n",
    "    data_dict['link'].append(item_dict.get('link', ''))\n",
    "    data_dict['pdf_link'].append(item_dict.get('pdf_link', ''))\n",
    "\n",
    "    for key, value in item_dict.items():\n",
    "        if key != 'link' and key != 'pdf_link':\n",
    "            data_dict[key] = data_dict.get(key, []) + [value]\n",
    "\n",
    "# Check the length of values for each key\n",
    "for key, values in data_dict.items():\n",
    "    print(f\"Key: {key}, Number of values: {len(values)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### organize the data in a csv , take all the keys as a new coloumn and set the value for eeach row \n",
    "output result was like : \n",
    "[{'Dimensioni esterne': '1535x740x825 mm', 'Dimensioni imballo': '1580x770x870 mm'}, {'Alimentazione': 'Elettrico', 'Capacità': '446 Lt', 'Gas refrigerante': 'R600A', \"Temperatura d'esercizio\": '+5 -25 °C'}, {'In dotazione': 'cestello portaoggetti superiore'}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data written to output_Ale.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import ast\n",
    "\n",
    "# Read the input CSV file\n",
    "input_file = '/Users/asifa/Desktop/Asifa/NLP KRISH/Ale_output.csv'\n",
    "output_file = 'output_Ale.csv'\n",
    "\n",
    "data = []\n",
    "\n",
    "# Read the CSV file\n",
    "with open(input_file, 'r', encoding='utf-8-sig') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    header = next(csv_reader)  # Read the header\n",
    "    data = list(csv_reader)  # Read the rest of the data\n",
    "\n",
    "# Extract unique keys from the third column dictionaries\n",
    "all_keys = set()\n",
    "for row in data:\n",
    "    info_dicts = ast.literal_eval(row[-1])  # List of dictionaries\n",
    "    for info_dict in info_dicts:\n",
    "        all_keys.update(info_dict.keys())\n",
    "\n",
    "# Prepare the header for the new CSV\n",
    "new_header = header[:-1]  # Exclude the third column\n",
    "new_header.extend(all_keys)  # Add unique keys as new columns\n",
    "\n",
    "# Extract and prepare the data for the new CSV\n",
    "new_data = []\n",
    "\n",
    "for row in data:\n",
    "    info_dicts = ast.literal_eval(row[-1])  # List of dictionaries\n",
    "    new_row = row[:-1]  # Exclude the third column\n",
    "\n",
    "    # Create a dictionary to hold values for each key\n",
    "    value_dict = {}\n",
    "    for info_dict in info_dicts:\n",
    "        value_dict.update(info_dict)\n",
    "\n",
    "    # Add values corresponding to each key in the new header\n",
    "    for key in all_keys:\n",
    "        new_row.append(value_dict.get(key, ''))\n",
    "\n",
    "    new_data.append(new_row)\n",
    "\n",
    "# Write the new CSV with transformed data\n",
    "with open(output_file, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "    csv_writer.writerow(new_header)\n",
    "    csv_writer.writerows(new_data)\n",
    "\n",
    "print(f\"Transformed data written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
