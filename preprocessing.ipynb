{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tokenization-convert paragraph-sentence-words\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import inflect\n",
    "from textblob import TextBlob\n",
    "import contractions\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Noise Reduction\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove special characters and punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    \n",
    "    # Expand Contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    \n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # Stop Word Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Convert Numerical Values to Words\n",
    "    p = inflect.engine()\n",
    "    def convert_currency(word):\n",
    "        if re.match(r'^\\d+$', word):\n",
    "            return inflect.engine().number_to_words(word)\n",
    "        elif re.match(r'^\\d+\\.\\d+$', word):\n",
    "            dollars, cents = word.split('.')\n",
    "            return f\"{inflect.engine().number_to_words(dollars)} dollars {inflect.engine().number_to_words(cents)} cents\"\n",
    "        else:\n",
    "            return word\n",
    "    \n",
    "   # words = [p.number_to_words(word) if word.replace('.', '', 1).isdigit() else word for word in words]\n",
    "    \n",
    "    # Correct Spelling Mistakes\n",
    "    blob = TextBlob(' '.join(words))\n",
    "    corrected_words = [str(blob.correct()) for word in words]\n",
    "    \n",
    "    # Reconstruct the preprocessed text\n",
    "    preprocessed_text = ' '.join(corrected_words)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Example text with various aspects\n",
    "example_text = \"Hi >! This is some <i>smpl</i> txt. There are 123 apples and $456.78 in my basket. Let's go!\"\n",
    "\n",
    "preprocessed_text = preprocess_text(example_text)\n",
    "print(\"Original Text:\")\n",
    "print(example_text)\n",
    "print(\"\\nPreprocessed Text:\")\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import inflect\n",
    "from textblob import TextBlob\n",
    "import contractions\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \n",
    "    \n",
    "    # Expand Contractions\n",
    "    text = contractions.fix(text)\n",
    "    \n",
    "    # Noise Reduction\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Remove special characters and punctuation\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    \n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lowercasing\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # Stop Word Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    \n",
    "    # Convert Numerical Values to Words\n",
    "    p = inflect.engine()\n",
    "    words = [p.number_to_words(word) if word.replace('.', '', 1).isdigit() else word for word in words]\n",
    "    \n",
    "    # Correct Spelling Mistakes\n",
    "    blob = TextBlob(' '.join(words))\n",
    "    corrected_words = [str(blob.correct()) for word in words]\n",
    "    \n",
    "    # Reconstruct the preprocessed text\n",
    "    preprocessed_text = ' '.join(corrected_words)\n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "# Example text with various aspects\n",
    "text = \"Hi, <b>world</b>! This is some <i>smpl</i> txt. There are 123 apples and $456.78 in my basket. Let's go!can't wait\"\n",
    "\n",
    "preprocessed_text = preprocess_text(text)\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"\\nPreprocessed Text:\")\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/asifa/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/asifa/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/asifa/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world! This is some smpl txt. There are 123 apples and $456.78 in my basket. Let's go!can't\n",
      "Hello  world  This is some smpl txt  There are 123 apples and $456 78 in my basket  Let's go can't\n",
      "Hello world This is some smpl txt There are 123 apples and $456 78 in my basket Let's go can't\n",
      "Hello world This is some smpl txt There are 123 apples and $456 78 in my basket Let us go cannot\n",
      "['Hello', 'world', 'This', 'is', 'some', 'smpl', 'txt', 'There', 'are', '123', 'apples', 'and', '$', '456', '78', 'in', 'my', 'basket', 'Let', 'us', 'go', 'can', 'not']\n",
      "['hello', 'world', 'this', 'is', 'some', 'smpl', 'txt', 'there', 'are', '123', 'apples', 'and', '$', '456', '78', 'in', 'my', 'basket', 'let', 'us', 'go', 'can', 'not']\n",
      "['hello', 'world', 'smpl', 'txt', '123', 'apples', '$', '456', '78', 'basket', 'let', 'us', 'go']\n",
      "['hello', 'world', 'smpl', 'txt', 'one hundred and twenty-three', 'apples', '$', 'four hundred and fifty-six', 'seventy-eight', 'basket', 'let', 'us', 'go']\n",
      "['hello', 'world', 'smpl', 'txt', 'one hundred and twenty-thre', 'appl', '$', 'four hundred and fifty-six', 'seventy-eight', 'basket', 'let', 'us', 'go']\n",
      "['hello', 'world', 'smpl', 'txt', 'one hundred and twenty-thre', 'appl', '$', 'four hundred and fifty-six', 'seventy-eight', 'basket', 'let', 'u', 'go']\n",
      "['hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go', 'hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go', 'hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go', 'hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go', 'hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go', 'hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go', 'hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go', 'hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go', 'hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go', 'hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go', 'hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go', 'hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go', 'hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go']\n",
      "Original Text:\n",
      "<p>Hello, <b>world</b>! This is some <i>smpl</i> txt. There are 123 apples and $456.78 in my basket. Let's go!can't\n",
      "\n",
      "Preprocessed Text:\n",
      "hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go hello world small txt one hundred and twenty-the apply $ four hundred and fifty-six seventy-eight basket let u go\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from textblob import TextBlob\n",
    "import contractions\n",
    "import inflect\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Noise Reduction\n",
    "    text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "    print(text)\n",
    "    text = re.sub(r'[^\\w\\s$\\'-]', ' ', text)  # Remove special characters and punctuation (except $, ', and -)\n",
    "    print(text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
    "    print(text)\n",
    "    \n",
    "    # Expand Contractions\n",
    "    text = contractions.fix(text)\n",
    "    print(text)\n",
    "    \n",
    "    # Tokenization\n",
    "    words = word_tokenize(text)\n",
    "    print(words)\n",
    "    \n",
    "    # Lowercasing\n",
    "    words = [word.lower() for word in words]\n",
    "    print(words)\n",
    "    \n",
    "    # Stop Word Removal\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in words if word not in stop_words]\n",
    "    print(words)\n",
    "    \n",
    "     # Convert Numerical Values to Words\n",
    "    words = [word if not re.match(r'^\\d+(\\.\\d+)?$', word) else convert_currency(word) for word in words]\n",
    "    print(words)\n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    print(words)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    print(words)\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Correct Spelling Mistakes\n",
    "    blob = TextBlob(' '.join(words))\n",
    "    corrected_words = [str(blob.correct()) for word in words]\n",
    "    print(corrected_words)\n",
    "    \n",
    "    # Reconstruct the preprocessed text\n",
    "    preprocessed_text = ' '.join(corrected_words)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return preprocessed_text\n",
    "\n",
    "def convert_currency(word):\n",
    "    if re.match(r'^\\d+$', word):\n",
    "        return inflect.engine().number_to_words(word)\n",
    "    elif re.match(r'^\\d+\\.\\d+$', word):\n",
    "        dollars, cents = word.split('.')\n",
    "        return f\"{inflect.engine().number_to_words(dollars)} dollars {inflect.engine().number_to_words(cents)} cents\"\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "# Example text with various aspects\n",
    "example_text = \"<p>Hello, <b>world</b>! This is some <i>smpl</i> txt. There are 123 apples and $456.78 in my basket. Let's go!can't\"\n",
    "\n",
    "preprocessed_text = preprocess_text(example_text)\n",
    "print(\"Original Text:\")\n",
    "print(example_text)\n",
    "print(\"\\nPreprocessed Text:\")\n",
    "print(preprocessed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "Thre are some speling mistaks in this text.\n",
      "\n",
      "Corrected Text with Spelling Mistakes Fixed:\n",
      "The are some spelling mistake in this text.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "# Example text with spelling mistakes\n",
    "text = \"Thre are some speling mistaks in this text.\"\n",
    "\n",
    "# Create a TextBlob object\n",
    "blob = TextBlob(text)\n",
    "\n",
    "# Correct spelling mistakes\n",
    "corrected_text = str(blob.correct())\n",
    "\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "\n",
    "print(\"\\nCorrected Text with Spelling Mistakes Fixed:\")\n",
    "print(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
