{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the Dataset\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "messages = pd.read_csv('/Users/asifa/Desktop/Asifa/NLP KRISH/SMSSpamCollection_new.csv', sep='\\t',\n",
    "                           names=[\"label\", \"message\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1002)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages#Data cleaning and preprocessing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it takes 8.5 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for i in range(0, len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri wkli comp win fa cup final tkt st may text fa receiv entri question std txt rate c appli',\n",
       " 'u dun say earli hor u c alreadi say',\n",
       " 'nah think goe usf live around though']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it takes 0.6 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []  # Initialize an empty list to store the processed reviews\n",
    "\n",
    "# Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Create a PorterStemmer instance for stemming\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# Loop through each entry in the 'message' column of your dataset\n",
    "for i in range(len(messages)):\n",
    "    # Extract the 'message' content and replace non-alphabetic characters with spaces\n",
    "    review = re.sub('[^A-Za-z]', ' ', messages['message'][i])\n",
    "    \n",
    "    # Convert the review to lowercase\n",
    "    review = review.lower()\n",
    "    \n",
    "    # Split the review into a list of words\n",
    "    review = review.split()\n",
    "    \n",
    "    # Initialize an empty list to store the processed words in the current review\n",
    "    processed_review = []\n",
    "    \n",
    "    # Loop through each word in the review\n",
    "    for word in review:\n",
    "        # Check if the word is not in the set of stopwords\n",
    "        if word not in stop_words:\n",
    "            # Apply stemming to the word\n",
    "            stemmed_word = ps.stem(word)\n",
    "            \n",
    "            # Append the stemmed word to the processed_review list\n",
    "            processed_review.append(stemmed_word)\n",
    "    \n",
    "    # Join the processed words to form the processed review\n",
    "    processed_review = ' '.join(processed_review)\n",
    "    \n",
    "    # Append the processed review to the corpus\n",
    "    corpus.append(processed_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri wkli comp win fa cup final tkt st may text fa receiv entri question std txt rate c appli',\n",
       " 'u dun say earli hor u c alreadi say',\n",
       " 'nah think goe usf live around though']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "for i in range(len(messages)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', messages['message'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    \n",
    "    filtered_review = [ps.stem(word) for word in review if word not in stop_words]\n",
    "    processed_review = ' '.join(filtered_review)\n",
    "    \n",
    "    corpus.append(processed_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating bag of word models\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=2500, binary = True, ngram_range=(2,2))\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ok lar joke wif u oni'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2500)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
